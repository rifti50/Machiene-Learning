{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":420,"sourceType":"datasetVersion","datasetId":19}],"dockerImageVersionId":30698,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"source":"<a href=\"https://www.kaggle.com/code/rifti50/knn-naive-bayes-on-iris-dataset?scriptVersionId=175695552\" target=\"_blank\"><img align=\"left\" alt=\"Kaggle\" title=\"Open in Kaggle\" src=\"https://kaggle.com/static/images/open-in-kaggle.svg\"></a>","metadata":{},"cell_type":"markdown"},{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","_kg_hide-input":true,"execution":{"iopub.status.busy":"2024-05-04T20:59:00.805414Z","iopub.execute_input":"2024-05-04T20:59:00.805937Z","iopub.status.idle":"2024-05-04T20:59:01.320953Z","shell.execute_reply.started":"2024-05-04T20:59:00.805901Z","shell.execute_reply":"2024-05-04T20:59:01.319806Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Contents\n*  **[ 1. Plots](#plot)**\n\n*  **[ 2. KNN Model Train](#Model_Train)**\n\n*  **[ 3. Checking various values of Hyper-parameter](#values_of_k)**\n\n*  **[ 4. Spliting Dataset to apply Naive Bayes](#Split)**\n\n*  **[ 5. Calling the Naive Bayers classifiers](#NB)**\n\n*  **[6. Fitting train data to Classifiers](#Fit)**\n\n*  **[ 7. Testing the accuracy of various model](#Test)**\n\n*  **[ 8. Decision](#Decision)**","metadata":{}},{"cell_type":"code","source":"# Importing necessary libraries \nfrom sklearn import metrics\nimport numpy as np # Linear Algebra\nimport pandas as pd # Handling Data\nimport matplotlib.pyplot as plt # Visualization\nimport seaborn as sns # Visualization\n\nfrom sklearn.pipeline import Pipeline # Data Pipeline\nfrom sklearn.impute import SimpleImputer # Imputation / Handling missing values\nfrom sklearn.preprocessing import StandardScaler # Scalling data\nfrom sklearn.feature_selection import SelectKBest # Selecting features\nfrom sklearn.decomposition import PCA # dimensionality reduction\nfrom sklearn.model_selection import train_test_split # splitting data\nfrom sklearn.ensemble import RandomForestClassifier # Classifier\nfrom sklearn.neighbors import KNeighborsClassifier # Classifier\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.metrics import accuracy_score\nfrom sklearn.ensemble import ExtraTreesClassifier # Feature selection\nfrom sklearn.metrics import classification_report, confusion_matrix # Performance Measures\nfrom sklearn.model_selection import GridSearchCV # Hyperparameter tuning\nfrom sklearn.model_selection import RandomizedSearchCV # Hyperparameter tuning\nfrom scipy.stats import randint # Random Integer\n\nimport warnings # for mitigating warnings\nwarnings.filterwarnings('ignore') # same as above","metadata":{"execution":{"iopub.status.busy":"2024-05-04T20:59:01.323533Z","iopub.execute_input":"2024-05-04T20:59:01.324807Z","iopub.status.idle":"2024-05-04T20:59:02.564785Z","shell.execute_reply.started":"2024-05-04T20:59:01.324755Z","shell.execute_reply":"2024-05-04T20:59:02.563595Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df= pd.read_csv('/kaggle/input/iris/Iris.csv')\ndf.sample(4)","metadata":{"execution":{"iopub.status.busy":"2024-05-04T20:59:02.566896Z","iopub.execute_input":"2024-05-04T20:59:02.567363Z","iopub.status.idle":"2024-05-04T20:59:02.617045Z","shell.execute_reply.started":"2024-05-04T20:59:02.567321Z","shell.execute_reply":"2024-05-04T20:59:02.615579Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df.info()","metadata":{"execution":{"iopub.status.busy":"2024-05-04T20:59:02.619732Z","iopub.execute_input":"2024-05-04T20:59:02.620092Z","iopub.status.idle":"2024-05-04T20:59:02.643679Z","shell.execute_reply.started":"2024-05-04T20:59:02.620063Z","shell.execute_reply":"2024-05-04T20:59:02.64231Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df.rename(columns=str.lower,inplace=True)","metadata":{"execution":{"iopub.status.busy":"2024-05-04T20:59:02.64541Z","iopub.execute_input":"2024-05-04T20:59:02.645764Z","iopub.status.idle":"2024-05-04T20:59:02.655842Z","shell.execute_reply.started":"2024-05-04T20:59:02.645736Z","shell.execute_reply":"2024-05-04T20:59:02.654708Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df.head()","metadata":{"execution":{"iopub.status.busy":"2024-05-04T20:59:02.657377Z","iopub.execute_input":"2024-05-04T20:59:02.657945Z","iopub.status.idle":"2024-05-04T20:59:02.677412Z","shell.execute_reply.started":"2024-05-04T20:59:02.65791Z","shell.execute_reply":"2024-05-04T20:59:02.676212Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## 1. Plot\n#### Creating a violinplot to check the differences\nThe thinner part denotes that there is less density whereas the fatter part shows higher density","metadata":{}},{"cell_type":"code","source":"plt.figure(figsize=(18,12)) #plot\nplt.subplot(2,2,1)\nsns.violinplot(x='species',y='sepallengthcm',data=df)\nplt.subplot(2,2,2)\nsns.violinplot(x='species',y='sepalwidthcm',data=df)\nplt.subplot(2,2,3)\nsns.violinplot(x='species',y='petallengthcm',data=df)\nplt.subplot(2,2,4)\nsns.violinplot(x='species',y='petalwidthcm',data=df)","metadata":{"execution":{"iopub.status.busy":"2024-05-04T20:59:02.679147Z","iopub.execute_input":"2024-05-04T20:59:02.679508Z","iopub.status.idle":"2024-05-04T20:59:04.010057Z","shell.execute_reply.started":"2024-05-04T20:59:02.67948Z","shell.execute_reply":"2024-05-04T20:59:04.009098Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Creating Correlation Heatmap","metadata":{}},{"cell_type":"code","source":"df_corr=df.drop(columns=['species','id'])\nplt.figure(figsize=(14,8))\nsns.heatmap(df_corr.corr(),annot=True)\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2024-05-04T20:59:04.011271Z","iopub.execute_input":"2024-05-04T20:59:04.012221Z","iopub.status.idle":"2024-05-04T20:59:04.421715Z","shell.execute_reply.started":"2024-05-04T20:59:04.012188Z","shell.execute_reply":"2024-05-04T20:59:04.420445Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## 2. KNN Model Train\n#### Let's Train the iris data on KNN model","metadata":{}},{"cell_type":"code","source":"#Model_Train\ntraindf_x=df[['sepallengthcm','sepalwidthcm','petallengthcm','petalwidthcm']]\ntraindf_y=df['species']","metadata":{"execution":{"iopub.status.busy":"2024-05-04T20:59:04.423316Z","iopub.execute_input":"2024-05-04T20:59:04.423795Z","iopub.status.idle":"2024-05-04T20:59:04.433317Z","shell.execute_reply.started":"2024-05-04T20:59:04.423742Z","shell.execute_reply":"2024-05-04T20:59:04.43218Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model=KNeighborsClassifier(n_neighbors=3) # Value of hyper perameter is 3\nmodel.fit(traindf_x,traindf_y)","metadata":{"execution":{"iopub.status.busy":"2024-05-04T20:59:04.437542Z","iopub.execute_input":"2024-05-04T20:59:04.437907Z","iopub.status.idle":"2024-05-04T20:59:04.452424Z","shell.execute_reply.started":"2024-05-04T20:59:04.437877Z","shell.execute_reply":"2024-05-04T20:59:04.451018Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"prediction=model.predict(traindf_x)\nmodel.score(traindf_x,traindf_y)","metadata":{"execution":{"iopub.status.busy":"2024-05-04T20:59:04.454169Z","iopub.execute_input":"2024-05-04T20:59:04.455148Z","iopub.status.idle":"2024-05-04T20:59:04.497327Z","shell.execute_reply.started":"2024-05-04T20:59:04.455118Z","shell.execute_reply":"2024-05-04T20:59:04.496175Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## 3. Checking various values of Hyper-parameter","metadata":{}},{"cell_type":"code","source":"a_index = list(range(1, 8))\naccuracy_scores = []  # Initialize an empty list to store accuracy scores\nb = [1, 2, 3, 4, 5, 6,7]\nfor i in list(range(1, 8)):\n    model = KNeighborsClassifier(n_neighbors=i) \n    model.fit(traindf_x, traindf_y)\n    prediction = model.predict(traindf_x)\n    accuracy_scores.append(metrics.accuracy_score(prediction, traindf_y))  # Append accuracy scores to the list\na = pd.Series(accuracy_scores)  # Create a series from the list of accuracy scores\nplt.plot(a_index, a)\nplt.xticks(b)\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2024-05-04T20:59:04.498874Z","iopub.execute_input":"2024-05-04T20:59:04.499296Z","iopub.status.idle":"2024-05-04T20:59:04.86059Z","shell.execute_reply.started":"2024-05-04T20:59:04.499256Z","shell.execute_reply":"2024-05-04T20:59:04.859466Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## 4. Spliting Dataset to apply Naive Bayes","metadata":{}},{"cell_type":"code","source":"x=traindf_x\ny=traindf_y\ntraindf_x,testdf_x,traindf_y,testdf_y= train_test_split(x,y,test_size=0.2,random_state=42)","metadata":{"execution":{"iopub.status.busy":"2024-05-04T20:59:04.862302Z","iopub.execute_input":"2024-05-04T20:59:04.863045Z","iopub.status.idle":"2024-05-04T20:59:04.871877Z","shell.execute_reply.started":"2024-05-04T20:59:04.862986Z","shell.execute_reply":"2024-05-04T20:59:04.870513Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df.shape #We will split the data into 80-20 section","metadata":{"execution":{"iopub.status.busy":"2024-05-04T20:59:04.8734Z","iopub.execute_input":"2024-05-04T20:59:04.874123Z","iopub.status.idle":"2024-05-04T20:59:04.886388Z","shell.execute_reply.started":"2024-05-04T20:59:04.874091Z","shell.execute_reply":"2024-05-04T20:59:04.885396Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(f\"Train data: {traindf_x.shape}\\n Test data: {testdf_x.shape}\")","metadata":{"execution":{"iopub.status.busy":"2024-05-04T20:59:04.887772Z","iopub.execute_input":"2024-05-04T20:59:04.888875Z","iopub.status.idle":"2024-05-04T20:59:04.898754Z","shell.execute_reply.started":"2024-05-04T20:59:04.888834Z","shell.execute_reply":"2024-05-04T20:59:04.897182Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## 5. Calling the Naive Bayers classifiers","metadata":{}},{"cell_type":"code","source":"from sklearn.naive_bayes import GaussianNB, MultinomialNB, BernoulliNB","metadata":{"execution":{"iopub.status.busy":"2024-05-04T20:59:04.901041Z","iopub.execute_input":"2024-05-04T20:59:04.901895Z","iopub.status.idle":"2024-05-04T20:59:04.911731Z","shell.execute_reply.started":"2024-05-04T20:59:04.901855Z","shell.execute_reply":"2024-05-04T20:59:04.910758Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Gaussian Naive Bayes\nGNB= GaussianNB()\n# Multinomial Naive Bayes\nMNB= MultinomialNB()\n# Bernoulli Naive Bayes\nBNB= BernoulliNB()","metadata":{"execution":{"iopub.status.busy":"2024-05-04T20:59:04.913203Z","iopub.execute_input":"2024-05-04T20:59:04.913888Z","iopub.status.idle":"2024-05-04T20:59:04.920247Z","shell.execute_reply.started":"2024-05-04T20:59:04.913836Z","shell.execute_reply":"2024-05-04T20:59:04.919042Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## 6. Fitting train data to Classifiers","metadata":{}},{"cell_type":"code","source":"GNB.fit(traindf_x,traindf_y)\nMNB.fit(traindf_x,traindf_y)\nBNB.fit(traindf_x,traindf_y)","metadata":{"execution":{"iopub.status.busy":"2024-05-04T20:59:04.921654Z","iopub.execute_input":"2024-05-04T20:59:04.922206Z","iopub.status.idle":"2024-05-04T20:59:04.951172Z","shell.execute_reply.started":"2024-05-04T20:59:04.922177Z","shell.execute_reply":"2024-05-04T20:59:04.949713Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## 7. Testing the accuracy of various model","metadata":{}},{"cell_type":"markdown","source":"#### Fitting of GNB model","metadata":{}},{"cell_type":"code","source":"print(f'Train Accuracy for GNB: {GNB.score(traindf_x,traindf_y)}')\nprint(f'Test Accuracy for GNB:{GNB.score(testdf_x,testdf_y)}')","metadata":{"execution":{"iopub.status.busy":"2024-05-04T20:59:04.953303Z","iopub.execute_input":"2024-05-04T20:59:04.954205Z","iopub.status.idle":"2024-05-04T20:59:04.966103Z","shell.execute_reply.started":"2024-05-04T20:59:04.954163Z","shell.execute_reply":"2024-05-04T20:59:04.965018Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### Fitting of MNB model","metadata":{}},{"cell_type":"code","source":"print(f'Accuracy for MNB: {MNB.score(traindf_x,traindf_y)}')\nprint(f'Test Accuracy for MNB:{MNB.score(testdf_x,testdf_y)}')","metadata":{"execution":{"iopub.status.busy":"2024-05-04T20:59:04.967452Z","iopub.execute_input":"2024-05-04T20:59:04.968394Z","iopub.status.idle":"2024-05-04T20:59:04.979162Z","shell.execute_reply.started":"2024-05-04T20:59:04.96834Z","shell.execute_reply":"2024-05-04T20:59:04.978047Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### Fitting of BNB model","metadata":{}},{"cell_type":"code","source":"print(f'Accuracy for BNB: {BNB.score(traindf_x,traindf_y)}')\nprint(f'Test Accuracy for BNB:{BNB.score(testdf_x,testdf_y)}')","metadata":{"execution":{"iopub.status.busy":"2024-05-04T20:59:04.982671Z","iopub.execute_input":"2024-05-04T20:59:04.983007Z","iopub.status.idle":"2024-05-04T20:59:04.997187Z","shell.execute_reply.started":"2024-05-04T20:59:04.982979Z","shell.execute_reply":"2024-05-04T20:59:04.995767Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### Fitting of KNN model","metadata":{}},{"cell_type":"code","source":"KNNmodel = KNeighborsClassifier(n_neighbors=3)  # Value of hyperparameter is 3\nKNNmodel.fit(traindf_x, traindf_y)\nprint(f'Accuracy for KNN:{KNNmodel.score(traindf_x, traindf_y)}')\nprint(f'Test Accuracy for KNN:{KNNmodel.score(testdf_x,testdf_y)}')","metadata":{"execution":{"iopub.status.busy":"2024-05-04T20:59:04.9989Z","iopub.execute_input":"2024-05-04T20:59:04.999618Z","iopub.status.idle":"2024-05-04T20:59:05.032848Z","shell.execute_reply.started":"2024-05-04T20:59:04.999583Z","shell.execute_reply":"2024-05-04T20:59:05.03187Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## 8. Decision\n### What is the suitable model for this DataSet?","metadata":{}},{"cell_type":"markdown","source":"#### According to the score of various model we can say that `Multi-nomial Naive Bayes` is the right model for this dataset because of it's discrete values.","metadata":{}}]}